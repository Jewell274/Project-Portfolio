{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tCD_SOMaNYe",
        "outputId": "073ae56d-a058-428e-dc55-25d2e6548ed8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.83)] [Waiting for headers] [C\u001b[0m\r                                                                               \rHit:2 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.83)] [Waiting for headers] [W\u001b[0m\r                                                                               \rHit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Ign:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "53 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n",
            "activity_recognition_exp.zip already exists. Skipping download.\n",
            "Extracting activity_recognition_exp.zip...\n",
            "Extraction complete.\n",
            "Exploring extracted directory structure:\n",
            "activity_data/\n",
            "  __MACOSX/\n",
            "    Activity recognition exp/\n",
            "      ._.DS_Store\n",
            "      ._readme.txt\n",
            "  Activity recognition exp/\n",
            "    .DS_Store\n",
            "    Phones_accelerometer.csv\n",
            "    Watch_gyroscope.csv\n",
            "    readme.txt\n",
            "    Phones_gyroscope.csv\n",
            "    Watch_accelerometer.csv\n",
            "Found 4 CSV file(s):\n",
            "activity_data/Activity recognition exp/Phones_accelerometer.csv\n",
            "activity_data/Activity recognition exp/Watch_gyroscope.csv\n",
            "activity_data/Activity recognition exp/Phones_gyroscope.csv\n",
            "activity_data/Activity recognition exp/Watch_accelerometer.csv\n",
            "\n",
            "Using CSV file: activity_data/Activity recognition exp/Phones_accelerometer.csv\n",
            "Loading data with Pandas...\n",
            "Loaded data with 13062475 records and 10 columns.\n",
            "Loaded data with Pandas in 25.7125 seconds\n",
            "Processing data with Pandas...\n",
            "Pandas processed data with 6 groups.\n",
            "Processed data with Pandas in 2.9073 seconds\n",
            "Loading data with Spark...\n",
            "Loaded data with Spark in 47.0442 seconds\n",
            "Processing data with Spark...\n",
            "Spark processed data with 7 groups.\n",
            "Processed data with Spark in 3.1878 seconds\n",
            "\n",
            "Performance Comparison:\n",
            "Pandas Load Time: 25.7125 seconds\n",
            "Spark Load Time: 47.0442 seconds\n",
            "Pandas Processing Time: 2.9073 seconds\n",
            "Spark Processing Time: 3.1878 seconds\n",
            "Pandas Total Time: 28.6198 seconds\n",
            "Spark Total Time: 50.2319 seconds\n"
          ]
        }
      ],
      "source": [
        "# Install necessary dependencies\n",
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import avg, count, min, max  # Import these functions\n",
        "import zipfile\n",
        "\n",
        "# Define the dataset URL and the file name\n",
        "dataset_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00344/Activity%20recognition%20exp.zip\"\n",
        "dataset_zip = \"activity_recognition_exp.zip\"\n",
        "\n",
        "# Download the dataset if it doesn't exist\n",
        "if not os.path.exists(dataset_zip):\n",
        "    print(f\"Downloading {dataset_zip}...\")\n",
        "    os.system(f\"wget -O {dataset_zip} {dataset_url}\")\n",
        "else:\n",
        "    print(f\"{dataset_zip} already exists. Skipping download.\")\n",
        "\n",
        "# Extract the dataset\n",
        "if os.path.exists(dataset_zip):\n",
        "    print(f\"Extracting {dataset_zip}...\")\n",
        "    with zipfile.ZipFile(dataset_zip, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"activity_data\")\n",
        "    print(\"Extraction complete.\")\n",
        "else:\n",
        "    print(f\"{dataset_zip} not found.\")\n",
        "\n",
        "# Function to explore directory structure\n",
        "def explore_directory(path, level=0):\n",
        "    print(f\"{'  ' * level}{os.path.basename(path)}/\")\n",
        "    if os.path.isdir(path):\n",
        "        for item in os.listdir(path):\n",
        "            item_path = os.path.join(path, item)\n",
        "            if os.path.isdir(item_path):\n",
        "                explore_directory(item_path, level + 1)\n",
        "            else:\n",
        "                print(f\"{'  ' * (level + 1)}{item}\")\n",
        "\n",
        "# Explore the extracted directory structure\n",
        "print(\"Exploring extracted directory structure:\")\n",
        "explore_directory(\"activity_data\")\n",
        "\n",
        "# Function to find CSV files recursively\n",
        "def find_csv_files(directory):\n",
        "    csv_files = []\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.endswith('.csv'):\n",
        "                csv_files.append(os.path.join(root, file))\n",
        "    return csv_files\n",
        "\n",
        "# Find CSV files\n",
        "csv_files = find_csv_files(\"activity_data\")\n",
        "if not csv_files:\n",
        "    raise FileNotFoundError(\"No CSV files found in the extracted data.\")\n",
        "\n",
        "print(f\"Found {len(csv_files)} CSV file(s):\")\n",
        "for file in csv_files:\n",
        "    print(file)\n",
        "\n",
        "# Use the first CSV file found\n",
        "dataset_csv = csv_files[0]\n",
        "print(f\"\\nUsing CSV file: {dataset_csv}\")\n",
        "\n",
        "# Load the dataset into a pandas DataFrame\n",
        "print(\"Loading data with Pandas...\")\n",
        "start_time = time.time()\n",
        "df_pandas = pd.read_csv(dataset_csv)\n",
        "pandas_load_time = time.time() - start_time\n",
        "print(f\"Loaded data with {len(df_pandas)} records and {len(df_pandas.columns)} columns.\")\n",
        "print(f\"Loaded data with Pandas in {pandas_load_time:.4f} seconds\")\n",
        "\n",
        "# Perform a more complex operation with Pandas and measure time\n",
        "print(\"Processing data with Pandas...\")\n",
        "start_time = time.time()\n",
        "result_pandas = df_pandas.groupby('gt').agg({\n",
        "    'x': 'mean',\n",
        "    'y': 'mean',\n",
        "    'z': 'mean'\n",
        "}).reset_index()\n",
        "result_pandas['count'] = df_pandas.groupby('gt').size().values\n",
        "pandas_time = time.time() - start_time\n",
        "print(f\"Pandas processed data with {len(result_pandas)} groups.\")\n",
        "print(f\"Processed data with Pandas in {pandas_time:.4f} seconds\")\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Activity Recognition Dataset Analysis\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .config(\"spark.executor.cores\", \"2\") \\\n",
        "    .config(\"spark.cores.max\", \"2\") \\\n",
        "    .master(\"local[2]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load the dataset in Spark\n",
        "print(\"Loading data with Spark...\")\n",
        "start_time = time.time()\n",
        "df_spark = spark.read.csv(dataset_csv, header=True, inferSchema=True).cache()\n",
        "df_spark.count()  # Force caching\n",
        "spark_load_time = time.time() - start_time\n",
        "print(f\"Loaded data with Spark in {spark_load_time:.4f} seconds\")\n",
        "\n",
        "# Perform the same complex operation with Spark and measure time\n",
        "print(\"Processing data with Spark...\")\n",
        "start_time = time.time()\n",
        "result_spark = df_spark.groupBy('gt').agg(\n",
        "    avg('x').alias('x'),\n",
        "    avg('y').alias('y'),\n",
        "    avg('z').alias('z'),\n",
        "    count('*').alias('count')\n",
        ")\n",
        "result_spark_count = result_spark.count()  # Trigger action to force execution\n",
        "spark_time = time.time() - start_time\n",
        "print(f\"Spark processed data with {result_spark_count} groups.\")\n",
        "print(f\"Processed data with Spark in {spark_time:.4f} seconds\")\n",
        "\n",
        "# Compare the performance\n",
        "print(f\"\\nPerformance Comparison:\")\n",
        "print(f\"Pandas Load Time: {pandas_load_time:.4f} seconds\")\n",
        "print(f\"Spark Load Time: {spark_load_time:.4f} seconds\")\n",
        "print(f\"Pandas Processing Time: {pandas_time:.4f} seconds\")\n",
        "print(f\"Spark Processing Time: {spark_time:.4f} seconds\")\n",
        "print(f\"Pandas Total Time: {pandas_load_time + pandas_time:.4f} seconds\")\n",
        "print(f\"Spark Total Time: {spark_load_time + spark_time:.4f} seconds\")\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}